{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HR Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore analyzing a Human Resource data set and create a neural network to predict whether an employee will leave or stay based on certain features we are given. We will be using Tensorflow, numpy  and pandas libraries to achieve this goal. The original python script is setup to accept command line flags to try different techniques I worked through this data with. If you are interested in seeing the results of full batch versus mini-batch training or Recitified Linear Unit(Relu) versus Exponential Linear Unit(Elu) activation functions then you can try it [here]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set our seeds for the environment and pull in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "tf.set_random_seed(7)\n",
    "\n",
    "init_data = pd.read_csv(\"./HR_comma_sep.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the data we are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 10 columns):\n",
      "satisfaction_level       14999 non-null float64\n",
      "last_evaluation          14999 non-null float64\n",
      "number_project           14999 non-null int64\n",
      "average_montly_hours     14999 non-null int64\n",
      "time_spend_company       14999 non-null int64\n",
      "Work_accident            14999 non-null int64\n",
      "left                     14999 non-null int64\n",
      "promotion_last_5years    14999 non-null int64\n",
      "sales                    14999 non-null object\n",
      "salary                   14999 non-null object\n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(init_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have 10 columns of features, 2 of those are of type object, with no null values in the data. Next we need to look at what type of data is in the two columns with the type object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales: 0    sales\n",
      "1    sales\n",
      "2    sales\n",
      "3    sales\n",
      "4    sales\n",
      "Name: sales, dtype: object\n",
      "Salary: 0       low\n",
      "1    medium\n",
      "2    medium\n",
      "3       low\n",
      "4       low\n",
      "Name: salary, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Sales: {0}\".format(init_data[\"sales\"][:5]))\n",
    "print(\"Salary: {0}\".format(init_data[\"salary\"][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it looks like the column \"sales\" is holding nominal data and the column \"salary\" is holding ordinal data. Lets see how many unique values we have in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sale categories: 10\n",
      "Unique salary categories: 3\n"
     ]
    }
   ],
   "source": [
    "sales_unique_n = init_data[\"sales\"].nunique()\n",
    "salary_unique_n = init_data[\"salary\"].nunique()\n",
    "print(\"Unique sale categories: {0}\".format(sales_unique_n))\n",
    "print(\"Unique salary categories: {0}\".format(salary_unique_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the nominal data has 10 categories and the ordinal data has 3 categories. Now we need to convert this data into something that our nerual network can work with. The way we are going to handle converting this data is by breaking these categories down into their own binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_unique_feature_names = init_data[\"sales\"].unique()\n",
    "salary_unique_feature_names = init_data[\"salary\"].unique()\n",
    "\n",
    "# Function to breakdown a category into individual binary features\n",
    "def break_down_features(feature_list, category, orig_data):\n",
    "    for name in feature_list:\n",
    "        orig_data[category+\"_\"+name] = [1 if x == name else 0 for _, x in enumerate(orig_data[category])]\n",
    "\n",
    "break_down_features(sales_unique_feature_names, \"sales\", init_data)\n",
    "break_down_features(salary_unique_feature_names, \"salary\", init_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have broken down the categories lets get rid of our original sales and salary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_data = init_data.drop([\"sales\", \"salary\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the feature we are wanting to predict is whether the employee has left or not, we should look at the percentages of left versus stayed in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.761917\n",
      "1    0.238083\n",
      "Name: left, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(init_data[\"left\"].value_counts() / len(init_data[\"left\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have about ~76% of people who stayed and ~24% of people who left. When we split our data into the training and test sets we want to try and maintain these percentages in our new distributions. Lets create a function that will do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stratified_split_data(data, ratio):\n",
    "    # Grab the data into its own category\n",
    "    stayed_data = data.loc[data[\"left\"] == 0]\n",
    "    left_data = data.loc[data[\"left\"] == 1]\n",
    "    # mix up the data\n",
    "    stayed_data = stayed_data.iloc[np.random.permutation(len(stayed_data))]\n",
    "    left_data = left_data.iloc[np.random.permutation(len(left_data))]\n",
    "    test_stayed_set_size = int(len(stayed_data) * ratio)\n",
    "    test_left_set_size = int(len(left_data) * ratio)\n",
    "    # Concatenate the partitioned data\n",
    "    train_set = pd.concat([stayed_data[test_stayed_set_size:], left_data[test_left_set_size:]], ignore_index=True)\n",
    "    test_set = pd.concat([stayed_data[:test_stayed_set_size], left_data[:test_left_set_size]], ignore_index=True)\n",
    "    # Now mix up the concatenated data\n",
    "    train_shuffled_indices = np.random.permutation(len(train_set))\n",
    "    test_shuffled_indices = np.random.permutation(len(test_set))\n",
    "    return train_set.iloc[train_shuffled_indices], test_set.iloc[test_shuffled_indices]\n",
    "\n",
    "train_set, test_set = stratified_split_data(init_data, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our stratified samples. However, just to make sure we can look at our training set percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.761917\n",
      "1    0.238083\n",
      "Name: left, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_set[\"left\"].value_counts() / len(train_set[\"left\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems our sample size's percentages match perfectly. So now lets split out our training set into the data and the data labels. Also lets grab the number of features we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = (train_set.drop(\"left\", axis=1)).values\n",
    "data_labels = train_set[\"left\"].values\n",
    "data_labels = data_labels.reshape([len(data_labels), 1])\n",
    "num_features = data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets start defining our model. The model we will create will have 2 hidden layers and an output layer. Lets start with defining the inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_init = tf.placeholder(tf.float32, [None, num_features])\n",
    "Y_init = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create our first hidden layer. Typically your hidden layers will use a Rectified Linear Unit(Relu) activation function in these cases, but in our case we will use an Exponential Linear Unit(Elu) activation function for its nice properties of reducing the bias shift effect on our network to have faster learning than Relu and for the fact that it acts like batch normalization without the computational complexity. We will also add the caveat of initalizing our weights and biases with a standard deviation of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_1 = tf.Variable(tf.truncated_normal([num_features, 10], stddev=0.01))\n",
    "b_1 = tf.Variable(tf.truncated_normal([10], stddev=0.01))\n",
    "\n",
    "layer_1 = tf.nn.elu(tf.add(tf.matmul(X_init, w_1), b_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second hidden layer will be the same but adjusted for the new input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_2 = tf.Variable(tf.truncated_normal([10, 8], stddev=0.01))\n",
    "b_2 = tf.Variable(tf.truncated_normal([8], stddev=0.01))\n",
    "\n",
    "layer_2 = tf.nn.elu(tf.add(tf.matmul(layer_1, w_2), b_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for out final output layer we will use the sigmoid function for our activation function because we want our output to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_3 = tf.Variable(tf.truncated_normal([8, 1], stddev=0.01))\n",
    "b_3 = tf.Variable(tf.truncated_normal([1], stddev=0.01))\n",
    "\n",
    "output_layer = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, w_3), b_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define our cost function, which we will use the cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(tf.multiply(Y_init, tf.log(output_layer)) + (1 - Y_init)*tf.log(1 - output_layer) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our optimizer we will use Adam with a learning rate of 1e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(1e-3).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do some tensorflow setup and define an array to store our loss function values when we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "loss_values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have played around with the values and have found a descent epoch and batch size numbers to use for our model. We will use 600 epochs and a batch size of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 600\n",
    "batch_size = 50\n",
    "count = len(data) # helper variable for our mini-batch training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets setup our training and then print out our final cost value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cost = 0.10991400480270386\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start_n = 0\n",
    "    c = None\n",
    "    while start_n < count:\n",
    "        sess.run(optimizer, feed_dict={X_init:data[start_n:(start_n + batch_size)], Y_init:data_labels[start_n:(start_n + batch_size)]})\n",
    "        start_n += batch_size\n",
    "    c = sess.run(cost, feed_dict={X_init:data, Y_init:data_labels})\n",
    "    loss_values.append(c)\n",
    "print(\"Final cost = {0}\".format(sess.run(cost, feed_dict={X_init:data, Y_init:data_labels})) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets grab our predictions of our training set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = sess.run(output_layer, feed_dict={X_init:data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a function that will give us a confusion matrix of our data and show us the F1 score and the total accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stayed True: 9055\n",
      "Stayed False: 88\n",
      "Left True: 2542\n",
      "Left False: 315\n",
      "Precision = 0.8897444872243612\n",
      "Recall = 0.9665399239543726\n",
      "F1 score = 0.9265536723163841\n",
      "Total Accuracy = 0.9664166666666667\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(pred_data, act_data, threshold=0.7):\n",
    "    stayed_true = 0\n",
    "    stayed_false = 0\n",
    "    left_true = 0\n",
    "    left_false = 0\n",
    "    for i in range(len(pred_data)):\n",
    "        if pred_data[i][0] >= threshold and act_data[i][0] == 1:\n",
    "            left_true += 1\n",
    "        elif pred_data[i][0] < threshold and act_data[i][0] == 1:\n",
    "            left_false += 1\n",
    "        elif pred_data[i][0] >= threshold and act_data[i][0] == 0:\n",
    "            stayed_false += 1\n",
    "        elif pred_data[i][0] < threshold and act_data[i][0] == 0:\n",
    "            stayed_true += 1\n",
    "    precision = left_true/np.max([1e-5, (left_true + left_false)])\n",
    "    recall = left_true/np.max([1e-5, (left_true + stayed_false)])\n",
    "    f1_score = 2*((precision*recall)/(precision+recall))\n",
    "    print(\"Stayed True: {0}\\nStayed False: {1}\\nLeft True: {2}\\nLeft False: {3}\".format(stayed_true, stayed_false, left_true, left_false))\n",
    "    print(\"Precision = {0}\".format(precision))\n",
    "    print(\"Recall = {0}\".format(recall))\n",
    "    print(\"F1 score = {0}\".format(f1_score))\n",
    "    print(\"Total Accuracy = {0}\".format((stayed_true+left_true)/(len(pred_data))) )\n",
    "\n",
    "confusion_matrix(predictions, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
